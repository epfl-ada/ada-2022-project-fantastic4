{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the plan:\n",
    "- Import the modules\n",
    "- Import the \"basic\" data (movies and characters datasets from CMU), clean it and save it\n",
    "- Extraction of the lemmatized version of the plot summaries from the corenlp processed data\n",
    "- Processing of the summaries according to the gender\n",
    "- Loading, cleaning of IMDb dataset\n",
    "- Matching CMU and IMDb datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from time import time\n",
    "import os\n",
    "import gzip\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pierre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pierre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Pierre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Pierre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Pierre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Pierre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download useful packages for nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File and folder names\n",
    "DATA_FOLDER = 'Data/'\n",
    "CHARACTER_DATASET = DATA_FOLDER + 'character.metadata.tsv'\n",
    "MOVIE_DATASET = DATA_FOLDER + 'movie.metadata.tsv'\n",
    "\n",
    "SUMMARIES_DATASET = DATA_FOLDER + 'plot_summaries.txt'\n",
    "NLP_FOLDER = DATA_FOLDER + 'corenlp_plot_summaries/'\n",
    "DEFAULT_COMPRESSION = 'gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data\n",
    "def load_metadata(path, column_names, header=None, low_memory=False):\n",
    "    return pd.read_table(path, header=header, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name columns\n",
    "columns_character = ['Wikipedia_movie_ID', 'Freebase_movie_ID', 'Movie_release_date', 'Character_name', 'Actor_date_of_birth', 'Actor_gender', 'Actor_height_meters', 'Actor_ethnicity_Freebase_ID', 'Actor_name', 'Actor_age_at_movie_release', 'Freebase_character_actor_map_ID', 'Freebase_character_ID', 'Freebase_actor_ID']\n",
    "columns_movie = ['Wikipedia_movie_ID', 'Freebase_movie_ID', 'Movie_name','Movie_release_date','Movie_box_office_revenue', 'Movie_runtime','Movie_languages','Movie_countries','Movie_genres' ]\n",
    "\n",
    "# Load data with correct column names\n",
    "characters = load_metadata(CHARACTER_DATASET,column_names=columns_character)\n",
    "movies = load_metadata(MOVIE_DATASET,column_names=columns_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summaries\n",
    "summaries = pd.read_csv(SUMMARIES_DATASET, sep='\\t', header=None, names=['id', 'plot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem of dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fix typos and absurd dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.loc[movies.Movie_release_date == '1010-12-02','Movie_release_date'] = '2010-12-02'\n",
    "characters.loc[characters.Movie_release_date == '1010-12-02','Movie_release_date'] = '2010-12-02'\n",
    "characters[characters.Actor_date_of_birth == '2050'] = '1971'\n",
    "characters = characters.drop(characters[characters.Actor_date_of_birth < '1500'].index)\n",
    "characters = characters.drop(characters[characters.Actor_date_of_birth > '2030'].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format of movie languages, genres and country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the format of languages, genres, country columns to a simpler format (in terms of utilisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_multiple(chain,deb,step):\n",
    "    '''Split the chain of characters at each \" encountered, and keep only the element in deb +i*step'''\n",
    "    res = chain.split('\"')[deb::step]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.loc[:,'Movie_genres'] = movies.Movie_genres.apply(format_multiple,deb=3,step=4)\n",
    "movies.loc[:,'Movie_countries'] = movies.Movie_countries.apply(format_multiple,deb=3,step=4)\n",
    "movies.loc[:,'Movie_languages'] = movies.Movie_languages.apply(format_multiple,deb=3,step=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13866 movies without Movie_languages (16.96% of the dataset)\n",
      "8154 movies without Movie_countries (9.98% of the dataset)\n",
      "2294 movies without Movie_genres (2.81% of the dataset)\n"
     ]
    }
   ],
   "source": [
    "keys = ['Movie_languages','Movie_countries','Movie_genres']\n",
    "for key in keys:\n",
    "    nb = len(movies[movies[key].apply(len) == 0])\n",
    "    print('{nb} movies without {key} ({percentage:.2f}% of the dataset)'.format(nb=nb,key=key, percentage=nb*100/len(movies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format for dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our study, we only keep the years from the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.Movie_release_date = pd.to_datetime(movies.Movie_release_date,format='%Y-%m-%d').dt.year\n",
    "characters.Movie_release_date = pd.to_datetime(characters.Movie_release_date,format='%Y-%m-%d').dt.year\n",
    "characters.Actor_date_of_birth = pd.to_datetime(characters.Actor_date_of_birth,format='%Y-%m-%d',utc=True,errors='coerce').dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pickle our data in order to reuse directly the cleaned data (and load it faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESTINATION = './Data/'\n",
    "EXT = '.pkl'\n",
    "to_pickle_data = [characters,movies]\n",
    "to_pickle_name = ['characters','movies']\n",
    "for i in range(len(to_pickle_data)):\n",
    "    to_pickle_data[i].to_pickle(DESTINATION+to_pickle_name[i]+EXT)\n",
    "\n",
    "# # To unpickle:\n",
    "# characters = pd.read_pickle(\"./Data/characters.pkl\") \n",
    "# movies = pd.read_pickle(\"./Data/movies.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pierre\\AppData\\Local\\Temp\\ipykernel_9264\\1742736688.py:3: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  genres = movies.Movie_genres.apply(pd.Series).stack().value_counts()[:topx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Drama', 'Comedy', 'Romance Film', 'Black-and-white', 'Action',\n",
      "       'Thriller', 'Short Film', 'World cinema', 'Crime Fiction', 'Indie'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Find 10 most common genres\n",
    "topx = 10\n",
    "genres = movies.Movie_genres.apply(pd.Series).stack().value_counts()[:topx]\n",
    "\n",
    "print(genres.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the list of movies id for each genre\n",
    "movies_id = {}\n",
    "for genre in genres.index:\n",
    "    movies_id[genre] = movies[movies.Movie_genres.apply(lambda x: genre in x)].Wikipedia_movie_ID.values\n",
    "\n",
    "# save results in a pickle file\n",
    "with open(DATA_FOLDER + 'movies_id_per_genres.pkl', 'wb') as f:\n",
    "    pickle.dump(movies_id, f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing the summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lemmatize data (for examples *'is'* becomes *'be'*) to be able to count words better. To do so, we used the `corenlp_plot_summaries` files, and exctracted from it the lemmatized versions of the movies summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to save the data\n",
    "LEMMATIZE_SUMMARIES = False # Takes ~7 mins to run (on i7-10875H CPU)\n",
    "\n",
    "if LEMMATIZE_SUMMARIES:\n",
    "    # Count the number of files in the directory\n",
    "    nb_files = 0\n",
    "    for filename in os.listdir(NLP_FOLDER):\n",
    "        path = os.path.join(NLP_FOLDER, filename)\n",
    "        nb_files += 1\n",
    "    print('Number of summaries:',nb_files)\n",
    "\n",
    "    ext = '.xml.gz' # Extension name\n",
    "    dico_processed_summmaries = {} # Dictionary to store the processed summaries\n",
    "    regex = r'<lemma>.*?</lemma>' # Expression to detect in the corenlp data <lemma>(word)</lemma>\n",
    "\n",
    "    deb = time() # Start timer\n",
    "    count = 0 # Counter\n",
    "\n",
    "    # Iteration over the files\n",
    "    for filename in os.listdir(NLP_FOLDER):\n",
    "        path = os.path.join(NLP_FOLDER, filename) # Path to the file\n",
    "        id_summary = path[len(NLP_FOLDER):-len(ext)] # id of the summary = filename without extension\n",
    "        summary = '' # String to store the summary\n",
    "\n",
    "        if os.path.isfile(path): # Checking if it is a file\n",
    "            with gzip.open(path, 'rb') as f: # Opening the .gz file\n",
    "                for line in f:\n",
    "                    txt = line.decode().strip() # Extract the line as txt\n",
    "                    for elt in re.finditer(regex,txt): # Find all the elements like regex\n",
    "                        summary += re.split('[><]',elt.group(0))[2].lower() + ' ' # Adding only the lemmatized word\n",
    "        \n",
    "        # Set the summary in the dictionary and increment the counter\n",
    "        dico_processed_summmaries[id_summary] = summary\n",
    "        count += 1\n",
    "\n",
    "        # Evolution of the process\n",
    "        if count%1000 == 0:\n",
    "            print('{processed}/{tot} files processed --> {perc:.1f}% ({t:.1f} seconds since deb)'.format(processed=count,tot=nb_files,perc=count/nb_files*100,t=time()-deb))\n",
    "    \n",
    "    # Pickle the file\n",
    "    with open(DATA_FOLDER + 'nlp_summaries.pkl', 'wb') as file:\n",
    "        pickle.dump(dico_processed_summmaries, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to extract the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 10000053\n",
      "Summary:\n",
      " Fur trapper Jean La B te paddle he canoe through wild water towards the settlement in order to sell a load of fur . at the settlement a steamboat be landing and the trader and he foster-child Eve , ar...\n"
     ]
    }
   ],
   "source": [
    "# Read the pickle file\n",
    "nlp_summaries = pd.read_pickle(DATA_FOLDER+'nlp_summaries.pkl')\n",
    "\n",
    "# Observe the first lemmatized summary\n",
    "for key,value in nlp_summaries.items():\n",
    "    print('Key:',key)\n",
    "    print('Summary:\\n',value[:200]+'...')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating sentences between sexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this part is to separate sentences between sexes to do a sentimental analysis later. To do so, we check if a feminine actor or the *'she'* pronoun is present in a sentence and add them to a new file. We do the same for a male actor and the *'he'* pronoun. Note that for example the sentence *'She hates him'* will become *'she hate he'* once lemmatized, which will be put in the feminine and maculine files\n",
    "\n",
    "This approach is not perfect, since for example in the sentences 'She likes butter. Indeed, the actress loves food.', only the first one will be added. It is not perfect, but the best solution we could think of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the characters\n",
    "characters_per_film = characters.copy()\n",
    "# Put the column in their correct type and lower chars\n",
    "characters_per_film['Wikipedia_movie_ID'] = characters_per_film['Wikipedia_movie_ID'].astype(int)\n",
    "characters_per_film['Character_name'] = characters_per_film['Character_name'].astype(str).apply(lambda x: x.lower())\n",
    "# Sort the dataframe by movie ID\n",
    "characters_per_film = characters_per_film.sort_values(by=['Wikipedia_movie_ID'])\n",
    "# Drio rows where the character name or the gender is empty\n",
    "characters_per_film = characters_per_film.dropna(subset=['Character_name', 'Actor_gender'])\n",
    "# Group the dataframe by movie ID\n",
    "characters_per_film = characters_per_film.groupby('Wikipedia_movie_ID')[['Wikipedia_movie_ID', 'Character_name', 'Actor_gender']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>plot_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27884</th>\n",
       "      <td>330</td>\n",
       "      <td>in order to prepare the role of a important ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26866</th>\n",
       "      <td>3217</td>\n",
       "      <td>after be pull through a time portal , Ash Will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28281</th>\n",
       "      <td>3333</td>\n",
       "      <td>the film follow two juxtapose family : the Nor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31566</th>\n",
       "      <td>3746</td>\n",
       "      <td>-lcb- -lcb- Hatnote -rcb- -rcb- in Los Angeles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31793</th>\n",
       "      <td>3837</td>\n",
       "      <td>in the American Old West of 1874 , constructio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                    plot_lemmatized\n",
       "27884   330  in order to prepare the role of a important ol...\n",
       "26866  3217  after be pull through a time portal , Ash Will...\n",
       "28281  3333  the film follow two juxtapose family : the Nor...\n",
       "31566  3746  -lcb- -lcb- Hatnote -rcb- -rcb- in Los Angeles...\n",
       "31793  3837  in the American Old West of 1874 , constructio..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataframe from lemmatized summaries\n",
    "df_lem = pd.DataFrame(list(nlp_summaries.items()), columns = ['id','plot_lemmatized'])\n",
    "# Put column in their correct type\n",
    "df_lem['id'] = df_lem['id'].astype(int)\n",
    "# Sort the dataframe by movie ID\n",
    "df_lem = df_lem.sort_values(by=['id'])\n",
    "# Show the first 5 rows\n",
    "df_lem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>330</td>\n",
       "      <td>In order to prepare the role of an important o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>3217</td>\n",
       "      <td>After being pulled through a time portal, Ash ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20555</th>\n",
       "      <td>3333</td>\n",
       "      <td>The film follows two juxtaposed families: the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39373</th>\n",
       "      <td>3746</td>\n",
       "      <td>{{Hatnote}} In Los Angeles, November 2019, ret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13609</th>\n",
       "      <td>3837</td>\n",
       "      <td>In the American Old West of 1874, construction...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               plot\n",
       "2100    330  In order to prepare the role of an important o...\n",
       "6038   3217  After being pulled through a time portal, Ash ...\n",
       "20555  3333   The film follows two juxtaposed families: the...\n",
       "39373  3746  {{Hatnote}} In Los Angeles, November 2019, ret...\n",
       "13609  3837  In the American Old West of 1874, construction..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a new dataframe with the movie ID and the plot\n",
    "summaries['id'] = summaries['id'].astype(int)\n",
    "# Sort the dataframe by movie ID\n",
    "summaries = summaries.sort_values(by=['id'])\n",
    "# Show the first 5 rows\n",
    "summaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to save the data\n",
    "SEPARATE_SENTENCES = False # Takes ~1 min 20 to run (on i7-10875H CPU)\n",
    "\n",
    "if SEPARATE_SENTENCES:\n",
    "    # Imports\n",
    "    count = 0\n",
    "    count_ignored = 0\n",
    "    dico_male = {}\n",
    "    dico_female = {}\n",
    "    dico_both = {}\n",
    "    regexp = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "    # Loop on subgroups\n",
    "    for _, group in characters_per_film:\n",
    "        # Get the movie id\n",
    "        movie_id = group['Wikipedia_movie_ID'].iloc[0]\n",
    "        female_sentences = []\n",
    "        male_sentences = []\n",
    "        both_sentences = []\n",
    "\n",
    "        # Check if wikipedia movie id is in the summaries\n",
    "        if (movie_id in df_lem['id'].values) and (movie_id in summaries['id'].values):\n",
    "            index_lem = df_lem[df_lem['id'] == movie_id].index[0] # Take the correct index\n",
    "            index_real = summaries[summaries['id'] == movie_id].index[0] # Take the correct index\n",
    "            plot_lem = df_lem['plot_lemmatized'][index_lem] # Take the correct plot\n",
    "            plot_real = summaries['plot'][index_real] # Take the correct plot\n",
    "            sentences_lem = plot_lem.split('.') # Split into sentences\n",
    "            sentences_real = plot_real.split('.') # Split into sentences\n",
    "\n",
    "            if len(sentences_lem) != len(sentences_real):\n",
    "                count_ignored += 1\n",
    "                continue\n",
    "\n",
    "            # Loop on sentences\n",
    "            for sentence_i in range(len(sentences_lem)):\n",
    "                # tokens = regexp.tokenize(sentences_lem[sentence_i])\n",
    "                # Loop on characters\n",
    "                sentence = sentences_lem[sentence_i]\n",
    "                for character in group['Character_name']:\n",
    "                    if character in sentences_lem[sentence_i]:     \n",
    "                        # Find the sex of the character\n",
    "                        gender = group[group['Character_name'] == character].Actor_gender.values[0]\n",
    "                        # Replace Character name by pronoun\n",
    "                        if gender == 'M':\n",
    "                            sentence = sentence.replace(character, 'he')\n",
    "                        elif gender == 'F':\n",
    "                            sentence = sentence.replace(character, 'she')\n",
    "\n",
    "                tokens = regexp.tokenize(sentence)\n",
    "\n",
    "                # Find potential pronouns discriminative on gender\n",
    "                he_index = False\n",
    "                she_index = False\n",
    "                for token in tokens:\n",
    "                    if token == 'he':\n",
    "                        he_index = True\n",
    "                    elif token == 'she':\n",
    "                        she_index = True\n",
    "\n",
    "                # Check where to append the sentence\n",
    "                if (she_index and he_index):\n",
    "                    both_sentences.append(sentences_real[sentence_i])\n",
    "\n",
    "                elif he_index:\n",
    "                    male_sentences.append(sentences_real[sentence_i])\n",
    "\n",
    "                elif she_index:\n",
    "                    female_sentences.append(sentences_real[sentence_i])\n",
    "\n",
    "\n",
    "        # Store in dictionary and increment counter\n",
    "        if len(male_sentences) > 0:\n",
    "            dico_male[movie_id] = male_sentences\n",
    "        if len(female_sentences) > 0:\n",
    "            dico_female[movie_id] = female_sentences\n",
    "        if len(both_sentences) > 0:\n",
    "            dico_both[movie_id] = both_sentences\n",
    "        count += 1\n",
    "\n",
    "        # Evolution of the process\n",
    "        if count%1000 == 0:\n",
    "            print('{processed} files processed'.format(processed=count))\n",
    "\n",
    "    print('Ignored {count} files'.format(count=count_ignored))\n",
    "\n",
    "    # Pickle the file\n",
    "    with open(DATA_FOLDER + 'male_sentences.pkl', 'wb') as file:\n",
    "        pickle.dump(dico_male, file, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "    with open(DATA_FOLDER + 'female_sentences.pkl', 'wb') as file:\n",
    "        pickle.dump(dico_female, file, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "    with open(DATA_FOLDER + 'both_sentences.pkl', 'wb') as file:\n",
    "        pickle.dump(dico_both, file, protocol=pickle.HIGHEST_PROTOCOL)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to save the data\n",
    "SEPARATE_SENTENCES_LEMMATIZE = False # Takes ~1 min 05 to run (on i7-10875H CPU)\n",
    "\n",
    "if SEPARATE_SENTENCES_LEMMATIZE:\n",
    "    # Imports\n",
    "    count = 0\n",
    "    dico_male = {}\n",
    "    dico_female = {}\n",
    "    dico_both = {}\n",
    "    regexp = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "    # Loop on subgroups\n",
    "    for _, group in characters_per_film:\n",
    "        # Get the movie id\n",
    "        movie_id = group['Wikipedia_movie_ID'].iloc[0]\n",
    "        female_sentences = []\n",
    "        male_sentences = []\n",
    "        both_sentences = []\n",
    "\n",
    "        # Check if wikipedia movie id is in the summaries\n",
    "        if (movie_id in df_lem['id'].values):\n",
    "            index_lem = df_lem[df_lem['id'] == movie_id].index[0] # Take the correct index\n",
    "            plot_lem = df_lem['plot_lemmatized'][index_lem] # Take the correct plot\n",
    "            sentences_lem = plot_lem.split('.') # Split into sentences\n",
    "\n",
    "            # Loop on sentences\n",
    "            for sentence in sentences_lem:\n",
    "                # Loop on characters\n",
    "                for character in group['Character_name']:\n",
    "                    if character in sentence:     \n",
    "                        # Find the sex of the character\n",
    "                        gender = group[group['Character_name'] == character].Actor_gender.values[0]\n",
    "                        # Replace Character name by pronoun\n",
    "                        if gender == 'M':\n",
    "                            sentence = sentence.replace(character, 'he')\n",
    "                        elif gender == 'F':\n",
    "                            sentence = sentence.replace(character, 'she')\n",
    "\n",
    "                tokens = regexp.tokenize(sentence)\n",
    "\n",
    "                # Find potential pronouns discriminative on gender\n",
    "                he_index = False\n",
    "                she_index = False\n",
    "                for token in tokens:\n",
    "                    if token == 'he':\n",
    "                        he_index = True\n",
    "                    elif token == 'she':\n",
    "                        she_index = True\n",
    "\n",
    "                # Check where to append the sentence\n",
    "                if (she_index and he_index):\n",
    "                    both_sentences.append(sentence)\n",
    "\n",
    "                elif he_index:\n",
    "                    male_sentences.append(sentence)\n",
    "\n",
    "                elif she_index:\n",
    "                    female_sentences.append(sentence)\n",
    "\n",
    "\n",
    "        # Store in dictionary and increment counter\n",
    "        if len(male_sentences) > 0:\n",
    "            dico_male[movie_id] = male_sentences\n",
    "        if len(female_sentences) > 0:\n",
    "            dico_female[movie_id] = female_sentences\n",
    "        if len(both_sentences) > 0:\n",
    "            dico_both[movie_id] = both_sentences\n",
    "        count += 1\n",
    "\n",
    "        # Evolution of the process\n",
    "        if count%1000 == 0:\n",
    "            print('{processed} files processed'.format(processed=count))\n",
    "\n",
    "    # Pickle the file\n",
    "    with open(DATA_FOLDER + 'male_sentences_lem.pkl', 'wb') as file:\n",
    "        pickle.dump(dico_male, file, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "    with open(DATA_FOLDER + 'female_sentences_lem.pkl', 'wb') as file:\n",
    "        pickle.dump(dico_female, file, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "    with open(DATA_FOLDER + 'both_sentences_lem.pkl', 'wb') as file:\n",
    "        pickle.dump(dico_both, file, protocol=pickle.HIGHEST_PROTOCOL)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 files processed\n",
      "2000 files processed\n",
      "3000 files processed\n",
      "4000 files processed\n",
      "5000 files processed\n",
      "6000 files processed\n",
      "7000 files processed\n",
      "8000 files processed\n",
      "9000 files processed\n",
      "10000 files processed\n",
      "11000 files processed\n",
      "12000 files processed\n",
      "13000 files processed\n",
      "14000 files processed\n",
      "15000 files processed\n",
      "16000 files processed\n",
      "17000 files processed\n",
      "18000 files processed\n",
      "19000 files processed\n",
      "20000 files processed\n",
      "21000 files processed\n",
      "22000 files processed\n",
      "23000 files processed\n",
      "24000 files processed\n",
      "25000 files processed\n",
      "26000 files processed\n",
      "27000 files processed\n",
      "28000 files processed\n",
      "29000 files processed\n",
      "30000 files processed\n",
      "31000 files processed\n",
      "32000 files processed\n",
      "33000 files processed\n",
      "34000 files processed\n",
      "35000 files processed\n",
      "36000 files processed\n",
      "37000 files processed\n",
      "38000 files processed\n",
      "39000 files processed\n",
      "40000 files processed\n",
      "41000 files processed\n",
      "42000 files processed\n",
      "43000 files processed\n",
      "44000 files processed\n",
      "45000 files processed\n",
      "46000 files processed\n",
      "47000 files processed\n",
      "48000 files processed\n",
      "49000 files processed\n",
      "50000 files processed\n",
      "51000 files processed\n",
      "52000 files processed\n",
      "53000 files processed\n",
      "54000 files processed\n",
      "55000 files processed\n",
      "56000 files processed\n",
      "57000 files processed\n",
      "58000 files processed\n",
      "59000 files processed\n",
      "60000 files processed\n",
      "61000 files processed\n",
      "62000 files processed\n",
      "63000 files processed\n",
      "64000 files processed\n",
      "65000 files processed\n",
      "66000 files processed\n",
      "67000 files processed\n",
      "68000 files processed\n",
      "69000 files processed\n",
      "70000 files processed\n",
      "71000 files processed\n",
      "72000 files processed\n",
      "73000 files processed\n",
      "74000 files processed\n",
      "75000 files processed\n",
      "76000 files processed\n",
      "77000 files processed\n",
      "78000 files processed\n",
      "79000 files processed\n",
      "80000 files processed\n",
      "81000 files processed\n",
      "82000 files processed\n",
      "83000 files processed\n",
      "84000 files processed\n",
      "85000 files processed\n",
      "86000 files processed\n",
      "87000 files processed\n",
      "88000 files processed\n",
      "89000 files processed\n",
      "90000 files processed\n",
      "91000 files processed\n",
      "92000 files processed\n",
      "93000 files processed\n",
      "94000 files processed\n",
      "95000 files processed\n",
      "96000 files processed\n",
      "97000 files processed\n",
      "98000 files processed\n",
      "99000 files processed\n",
      "100000 files processed\n",
      "101000 files processed\n"
     ]
    }
   ],
   "source": [
    "# Set to True to save the data\n",
    "SEPARATE_SENTENCES_LEMMATIZE_GENRES = True # Takes ~1 min 05 to run (on i7-10875H CPU)\n",
    "\n",
    "if SEPARATE_SENTENCES_LEMMATIZE_GENRES:\n",
    "    # Imports\n",
    "    count = 0\n",
    "    regexp = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "    for genre in genres.index:\n",
    "        movies_id[genre]\n",
    "\n",
    "        # Check if movies_id[genre] is in the character_per_film groupby\n",
    "        characters_genre = characters.copy()\n",
    "        # Put the column in their correct type and lower chars\n",
    "        characters_genre['Wikipedia_movie_ID'] = characters_genre['Wikipedia_movie_ID'].astype(int)\n",
    "        characters_genre['Character_name'] = characters_genre['Character_name'].astype(str).apply(lambda x: x.lower())\n",
    "        # Sort the dataframe by movie ID\n",
    "        characters_genre = characters_genre.sort_values(by=['Wikipedia_movie_ID'])\n",
    "        # Drio rows where the character name or the gender is empty\n",
    "        characters_genre = characters_genre.dropna(subset=['Character_name', 'Actor_gender'])\n",
    "        # Group the dataframe by movie ID\n",
    "        characters_genre = characters_genre[characters_genre['Wikipedia_movie_ID'].isin(movies_id[genre])]\n",
    "\n",
    "        characters_genre = characters_genre.groupby('Wikipedia_movie_ID')[['Wikipedia_movie_ID', 'Character_name', 'Actor_gender']]\n",
    "\n",
    "        dico_male = {}\n",
    "        dico_female = {}\n",
    "        dico_both = {}\n",
    "\n",
    "        # Loop on subgroups\n",
    "        for _, group in characters_genre:\n",
    "            # Get the movie id\n",
    "            movie_id = group['Wikipedia_movie_ID'].iloc[0]\n",
    "            female_sentences = []\n",
    "            male_sentences = []\n",
    "            both_sentences = []\n",
    "\n",
    "            # Check if wikipedia movie id is in the summaries\n",
    "            if (movie_id in df_lem['id'].values):\n",
    "                index_lem = df_lem[df_lem['id'] == movie_id].index[0] # Take the correct index\n",
    "                plot_lem = df_lem['plot_lemmatized'][index_lem] # Take the correct plot\n",
    "                sentences_lem = plot_lem.split('.') # Split into sentences\n",
    "\n",
    "                # Loop on sentences\n",
    "                for sentence in sentences_lem:\n",
    "                    # Loop on characters\n",
    "                    for character in group['Character_name']:\n",
    "                        if character in sentence:     \n",
    "                            # Find the sex of the character\n",
    "                            gender = group[group['Character_name'] == character].Actor_gender.values[0]\n",
    "                            # Replace Character name by pronoun\n",
    "                            if gender == 'M':\n",
    "                                sentence = sentence.replace(character, 'he')\n",
    "                            elif gender == 'F':\n",
    "                                sentence = sentence.replace(character, 'she')\n",
    "\n",
    "                    tokens = regexp.tokenize(sentence)\n",
    "\n",
    "                    # Find potential pronouns discriminative on gender\n",
    "                    he_index = False\n",
    "                    she_index = False\n",
    "                    for token in tokens:\n",
    "                        if token == 'he':\n",
    "                            he_index = True\n",
    "                        elif token == 'she':\n",
    "                            she_index = True\n",
    "\n",
    "                    # Check where to append the sentence\n",
    "                    if (she_index and he_index):\n",
    "                        both_sentences.append(sentence)\n",
    "\n",
    "                    elif he_index:\n",
    "                        male_sentences.append(sentence)\n",
    "\n",
    "                    elif she_index:\n",
    "                        female_sentences.append(sentence)\n",
    "\n",
    "\n",
    "            # Store in dictionary and increment counter\n",
    "            if len(male_sentences) > 0:\n",
    "                dico_male[movie_id] = male_sentences\n",
    "            if len(female_sentences) > 0:\n",
    "                dico_female[movie_id] = female_sentences\n",
    "            if len(both_sentences) > 0:\n",
    "                dico_both[movie_id] = both_sentences\n",
    "            count += 1\n",
    "\n",
    "            # Evolution of the process\n",
    "            if count%1000 == 0:\n",
    "                print('{processed} files processed'.format(processed=count))\n",
    "\n",
    "        # Pickle the file\n",
    "        with open(DATA_FOLDER + genre+'_male_sentences_lem.pkl', 'wb') as file:\n",
    "            pickle.dump(dico_male, file, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "        with open(DATA_FOLDER + genre+'_female_sentences_lem.pkl', 'wb') as file:\n",
    "            pickle.dump(dico_female, file, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "        with open(DATA_FOLDER + genre+'_both_sentences_lem.pkl', 'wb') as file:\n",
    "            pickle.dump(dico_both, file, protocol=pickle.HIGHEST_PROTOCOL)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse sentiments for each group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run it in the handling of data since it takes a long time to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentences</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>330</td>\n",
       "      <td>[In order to prepare the role of an important ...</td>\n",
       "      <td>In order to prepare the role of an important o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3217</td>\n",
       "      <td>[ However, Sheila is captured by a Flying Dead...</td>\n",
       "      <td>However, Sheila is captured by a Flying Deadi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3333</td>\n",
       "      <td>[ Elsie takes Cameron's mother, who has travel...</td>\n",
       "      <td>Elsie takes Cameron's mother, who has travele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3746</td>\n",
       "      <td>[ Sent to the Tyrell Corporation to ensure tha...</td>\n",
       "      <td>Sent to the Tyrell Corporation to ensure that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3947</td>\n",
       "      <td>[ Increasingly curious, Jeffrey enters Dorothy...</td>\n",
       "      <td>Increasingly curious, Jeffrey enters Dorothy'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4227</td>\n",
       "      <td>[ The widow , disdaining offers of marriage, d...</td>\n",
       "      <td>The widow , disdaining offers of marriage, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4231</td>\n",
       "      <td>[Buffy Summers  is introduced as a stereotypic...</td>\n",
       "      <td>Buffy Summers  is introduced as a stereotypica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4560</td>\n",
       "      <td>[ When an English soldier tries to rape Murron...</td>\n",
       "      <td>When an English soldier tries to rape Murron,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4726</td>\n",
       "      <td>[ Vicki and Knox attend a benefit at Wayne Man...</td>\n",
       "      <td>Vicki and Knox attend a benefit at Wayne Mano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4728</td>\n",
       "      <td>[ Selina survives the fall, but it causes a ps...</td>\n",
       "      <td>Selina survives the fall, but it causes a psy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                          sentences  \\\n",
       "0   330  [In order to prepare the role of an important ...   \n",
       "1  3217  [ However, Sheila is captured by a Flying Dead...   \n",
       "2  3333  [ Elsie takes Cameron's mother, who has travel...   \n",
       "3  3746  [ Sent to the Tyrell Corporation to ensure tha...   \n",
       "4  3947  [ Increasingly curious, Jeffrey enters Dorothy...   \n",
       "5  4227  [ The widow , disdaining offers of marriage, d...   \n",
       "6  4231  [Buffy Summers  is introduced as a stereotypic...   \n",
       "7  4560  [ When an English soldier tries to rape Murron...   \n",
       "8  4726  [ Vicki and Knox attend a benefit at Wayne Man...   \n",
       "9  4728  [ Selina survives the fall, but it causes a ps...   \n",
       "\n",
       "                                             summary  \n",
       "0  In order to prepare the role of an important o...  \n",
       "1   However, Sheila is captured by a Flying Deadi...  \n",
       "2   Elsie takes Cameron's mother, who has travele...  \n",
       "3   Sent to the Tyrell Corporation to ensure that...  \n",
       "4   Increasingly curious, Jeffrey enters Dorothy'...  \n",
       "5   The widow , disdaining offers of marriage, de...  \n",
       "6  Buffy Summers  is introduced as a stereotypica...  \n",
       "7   When an English soldier tries to rape Murron,...  \n",
       "8   Vicki and Knox attend a benefit at Wayne Mano...  \n",
       "9   Selina survives the fall, but it causes a psy...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import male sentences\n",
    "male_sentences_dict = pd.read_pickle(DATA_FOLDER + 'male_sentences.pkl')\n",
    "# Form a dataframe\n",
    "male_sentences = pd.DataFrame(list(male_sentences_dict.items()), columns = ['id','sentences'])\n",
    "# Create a new column that reconstructs the summary from the lemmatized sentences\n",
    "male_sentences['summary'] = male_sentences['sentences'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Import female sentences\n",
    "female_sentences_dict = pd.read_pickle(DATA_FOLDER + 'female_sentences.pkl')\n",
    "# Form a dataframe\n",
    "female_sentences = pd.DataFrame(list(female_sentences_dict.items()), columns = ['id','sentences'])\n",
    "# Create a new column that reconstructs the summary from the lemmatized sentences\n",
    "female_sentences['summary'] = female_sentences['sentences'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Import both sentences\n",
    "both_sentences_dict = pd.read_pickle(DATA_FOLDER + 'both_sentences.pkl')\n",
    "# Form a dataframe\n",
    "both_sentences = pd.DataFrame(list(both_sentences_dict.items()), columns = ['id','sentences'])\n",
    "# Create a new column that reconstructs the summary from the lemmatized sentences\n",
    "both_sentences['summary'] = both_sentences['sentences'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Show the first 5 rows of male sentences\n",
    "female_sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentences</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3217</td>\n",
       "      <td>[After being pulled through a time portal, Ash...</td>\n",
       "      <td>After being pulled through a time portal, Ash ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3333</td>\n",
       "      <td>[ The film follows two juxtaposed families: th...</td>\n",
       "      <td>The film follows two juxtaposed families: the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3746</td>\n",
       "      <td>[{{Hatnote}} In Los Angeles, November 2019, re...</td>\n",
       "      <td>{{Hatnote}} In Los Angeles, November 2019, ret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3837</td>\n",
       "      <td>[In the American Old West of 1874, constructio...</td>\n",
       "      <td>In the American Old West of 1874, construction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3947</td>\n",
       "      <td>[Jeffrey Beaumont  returns to his logging home...</td>\n",
       "      <td>Jeffrey Beaumont  returns to his logging home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4227</td>\n",
       "      <td>[ :By What Means Redmond Barry Acquired the St...</td>\n",
       "      <td>:By What Means Redmond Barry Acquired the Sty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4231</td>\n",
       "      <td>[ Benny is turned but Oliver is saved by Merri...</td>\n",
       "      <td>Benny is turned but Oliver is saved by Merric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4560</td>\n",
       "      <td>[In the 13th century, after several years of p...</td>\n",
       "      <td>In the 13th century, after several years of po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4726</td>\n",
       "      <td>[As a child, Bruce Wayne  witnesses his parent...</td>\n",
       "      <td>As a child, Bruce Wayne  witnesses his parents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4727</td>\n",
       "      <td>[When Batman  and Robin  get a tip that Commod...</td>\n",
       "      <td>When Batman  and Robin  get a tip that Commodo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                          sentences  \\\n",
       "0  3217  [After being pulled through a time portal, Ash...   \n",
       "1  3333  [ The film follows two juxtaposed families: th...   \n",
       "2  3746  [{{Hatnote}} In Los Angeles, November 2019, re...   \n",
       "3  3837  [In the American Old West of 1874, constructio...   \n",
       "4  3947  [Jeffrey Beaumont  returns to his logging home...   \n",
       "5  4227  [ :By What Means Redmond Barry Acquired the St...   \n",
       "6  4231  [ Benny is turned but Oliver is saved by Merri...   \n",
       "7  4560  [In the 13th century, after several years of p...   \n",
       "8  4726  [As a child, Bruce Wayne  witnesses his parent...   \n",
       "9  4727  [When Batman  and Robin  get a tip that Commod...   \n",
       "\n",
       "                                             summary  \n",
       "0  After being pulled through a time portal, Ash ...  \n",
       "1   The film follows two juxtaposed families: the...  \n",
       "2  {{Hatnote}} In Los Angeles, November 2019, ret...  \n",
       "3  In the American Old West of 1874, construction...  \n",
       "4  Jeffrey Beaumont  returns to his logging home ...  \n",
       "5   :By What Means Redmond Barry Acquired the Sty...  \n",
       "6   Benny is turned but Oliver is saved by Merric...  \n",
       "7  In the 13th century, after several years of po...  \n",
       "8  As a child, Bruce Wayne  witnesses his parents...  \n",
       "9  When Batman  and Robin  get a tip that Commodo...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "SAVE_SENTIMENTS = True # Takes ~1 min to run (on i7-10875H CPU)\n",
    "\n",
    "if SAVE_SENTIMENTS:\n",
    "    # Use nltk Vader to get the sentiment of the sentences\n",
    "    \n",
    "    analyzer =  SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Apply sentiments to plots\n",
    "    male_sentences['polarity'] = male_sentences['summary'].apply(lambda x: analyzer.polarity_scores(x))\n",
    "    female_sentences['polarity'] = female_sentences['summary'].apply(lambda x: analyzer.polarity_scores(x))\n",
    "    both_sentences['polarity'] = both_sentences['summary'].apply(lambda x: analyzer.polarity_scores(x))\n",
    "\n",
    "    # Pickle the file\n",
    "    with open(DATA_FOLDER + 'male_sentiments.pkl', 'wb') as file:\n",
    "        pickle.dump(male_sentences, file, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "    with open(DATA_FOLDER + 'female_sentiments.pkl', 'wb') as file:\n",
    "        pickle.dump(female_sentences, file, protocol=pickle.HIGHEST_PROTOCOL)   \n",
    "    with open(DATA_FOLDER + 'both_sentiments.pkl', 'wb') as file:\n",
    "        pickle.dump(both_sentences, file, protocol=pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriching the CMU dataset with IMDb dataset\n",
    "\n",
    "## Loading the data and first glimpse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the most useful datasets for the moment\n",
    "TITLE_BASICS_DATASET = DATA_FOLDER + 'title.basics.tsv.gz'\n",
    "TITLE_RATINGS_DATASET = DATA_FOLDER + 'title.ratings.tsv.gz'\n",
    "\n",
    "columns_title_basics = ['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult', 'startYear', 'endYear', 'runtimeMinutes', 'genres']\n",
    "columns_ratings = ['tconstIdentifier', 'averageRating', 'numVotes']\n",
    "\n",
    "CLEAN_DATA = False # True to clean again the data, False to use the already pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_DATA:\n",
    "    #Load title_basics\n",
    "    title_basics = load_metadata(TITLE_BASICS_DATASET, column_names=columns_title_basics)\n",
    "    print(\"length of title_basics: \", len(title_basics))\n",
    "    title_basics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_DATA:\n",
    "    #Load title_ratings\n",
    "    ratings = load_metadata(TITLE_RATINGS_DATASET, column_names=columns_ratings)\n",
    "    print(\"length of ratings: \", len(ratings))\n",
    "    ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_DATA:\n",
    "    #Create a new table with only titleType=movies (get rid of videos, tvshows, tvepisodes and short)\n",
    "    title_basics_movies = title_basics[title_basics[\"titleType\"] == \"movie\"]\n",
    "    #Remove the endYear column since movies are not concerned by thats\n",
    "    title_basics_movies_cleaned = title_basics_movies.drop(columns='endYear')\n",
    "    title_basics_movies_cleaned.replace('\\\\N',np.NaN,inplace=True) # replace \\N by NaN\n",
    "    # datetime format for dates\n",
    "    title_basics_movies_cleaned.startYear = pd.to_datetime(title_basics_movies_cleaned.startYear,format='%Y').dt.year \n",
    "    title_basics_movies_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_DATA:\n",
    "    #Pickle the data\n",
    "    to_pickle_data = title_basics_movies_cleaned\n",
    "    to_pickle_name = 'IMDb_title_movies'\n",
    "    to_pickle_data.to_pickle(DESTINATION+to_pickle_name+EXT)\n",
    "\n",
    "if not CLEAN_DATA: # for testing part\n",
    "    # load already pickled data\n",
    "    title_basics_movies_cleaned = pd.read_pickle(\"./Data/IMDb_title_movies.pkl\")\n",
    "    title_basics_movies_cleaned.startYear = pd.to_datetime(title_basics_movies_cleaned.startYear,format='%Y').dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging IMDb and CMU datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We match the movies from one dataset to the films on the other dataset on the movie name, as the ids are different.\n",
    "\n",
    "In order to avoid mismatched pairs due to a little variation in the titles, we matched films of the same year, with almost identical titles. We create a dictionnary that matches the index of matched films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_IMDb = title_basics_movies_cleaned.copy()\n",
    "copy_IMDb = copy_IMDb[copy_IMDb.startYear >= 1910]\n",
    "copy_CMU = movies.copy()\n",
    "copy_CMU.dropna(subset=['Movie_box_office_revenue', 'Movie_release_date'], inplace=True)\n",
    "copy_IMDb.dropna(subset= ['startYear'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(copy_CMU))\n",
    "print(len(copy_IMDb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "common_words = {'a','an','and','the','of','at','in'}\n",
    "punctuation = {'.',',','!',';','?',''}\n",
    "def compare(df1,df2,col1_title,col2_title,col1_year,col2_year,threshold = 0.8, delta_year=1):\n",
    "    matched = {}\n",
    "    count = 0\n",
    "    for idx1,row1 in df1.iterrows():\n",
    "        title1 = set(re.split('[ :,]',row1[col1_title].lower()))\n",
    "        title1 = title1.difference(punctuation)\n",
    "        y1 = row1[col1_year]\n",
    "        #for idx2,row2 in df2[df2[col2_year].isin([y1-delta_year+i for i in range(delta_year*2)])].iterrows():\n",
    "        for idx2,row2 in df2[df2[col2_year]==y1].iterrows():\n",
    "            title2 = set(re.split('[ :,]',row2[col2_title].lower()))\n",
    "            title2 = title2.difference(punctuation)\n",
    "            if len(title1 & title2)/(len(title1 | title2)) > threshold:\n",
    "                try:\n",
    "                    matched[idx1].append(idx2)\n",
    "                except KeyError:\n",
    "                    matched[idx1] = [idx2]\n",
    "        count += 1\n",
    "        if count == 100: # remove for the whole computation\n",
    "            break\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "deb = time()\n",
    "matched = compare(copy_CMU,copy_IMDb, 'Movie_name', 'primaryTitle', 'Movie_release_date', 'startYear')\n",
    "end = time()\n",
    "print('Time of execution:', end-deb)\n",
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the matching table\n",
    "with open(DATA_FOLDER + 'matching_table.pkl', 'wb') as file:\n",
    "        pickle.dump(matched, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a957044d351fb82484f120dca125b0411d05a6141ed71a6e42fbd3678d62e425"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
