{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the plan:\n",
    "- Import the modules\n",
    "- Import the \"basic\" data (movies and characters datasets from CMU), clean it and save it\n",
    "- Extraction of the lemmatized version of the plot summaries from the corenlp processed data\n",
    "- Processing of the summaries according to the gender\n",
    "- Loading, cleaning of IMDb dataset\n",
    "- Matching CMU and IMDb datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File and folder names\n",
    "DATA_FOLDER = 'Data/'\n",
    "CHARACTER_DATASET = DATA_FOLDER + 'character.metadata.tsv'\n",
    "MOVIE_DATASET = DATA_FOLDER + 'movie.metadata.tsv'\n",
    "\n",
    "SUMMARIES_DATASET = DATA_FOLDER + 'plot_summaries.txt'\n",
    "NLP_FOLDER = DATA_FOLDER + 'corenlp_plot_summaries/'\n",
    "DEFAULT_COMPRESSION = 'gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data\n",
    "def load_metadata(path, column_names, header=None, low_memory=False):\n",
    "    return pd.read_table(path, header=header, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name columns\n",
    "columns_character = ['Wikipedia_movie_ID', 'Freebase_movie_ID', 'Movie_release_date', 'Character_name', 'Actor_date_of_birth', 'Actor_gender', 'Actor_height_meters', 'Actor_ethnicity_Freebase_ID', 'Actor_name', 'Actor_age_at_movie_release', 'Freebase_character_actor_map_ID', 'Freebase_character_ID', 'Freebase_actor_ID']\n",
    "columns_movie = ['Wikipedia_movie_ID', 'Freebase_movie_ID', 'Movie_name','Movie_release_date','Movie_box_office_revenue', 'Movie_runtime','Movie_languages','Movie_countries','Movie_genres' ]\n",
    "\n",
    "# Load data with correct column names\n",
    "characters = load_metadata(CHARACTER_DATASET,column_names=columns_character)\n",
    "movies = load_metadata(MOVIE_DATASET,column_names=columns_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summaries\n",
    "with open(SUMMARIES_DATASET,'r', encoding='utf-8') as file:\n",
    "    summaries = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First glimpse at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we observe the movies dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81741\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia_movie_ID</th>\n",
       "      <th>Freebase_movie_ID</th>\n",
       "      <th>Movie_name</th>\n",
       "      <th>Movie_release_date</th>\n",
       "      <th>Movie_box_office_revenue</th>\n",
       "      <th>Movie_runtime</th>\n",
       "      <th>Movie_languages</th>\n",
       "      <th>Movie_countries</th>\n",
       "      <th>Movie_genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>975900</td>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>Ghosts of Mars</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>14010832.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>{\"/m/02h40lc\": \"English Language\"}</td>\n",
       "      <td>{\"/m/09c7w0\": \"United States of America\"}</td>\n",
       "      <td>{\"/m/01jfsb\": \"Thriller\", \"/m/06n90\": \"Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3196793</td>\n",
       "      <td>/m/08yl5d</td>\n",
       "      <td>Getting Away with Murder: The JonBenét Ramsey ...</td>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.0</td>\n",
       "      <td>{\"/m/02h40lc\": \"English Language\"}</td>\n",
       "      <td>{\"/m/09c7w0\": \"United States of America\"}</td>\n",
       "      <td>{\"/m/02n4kr\": \"Mystery\", \"/m/03bxz7\": \"Biograp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wikipedia_movie_ID Freebase_movie_ID  \\\n",
       "0              975900         /m/03vyhn   \n",
       "1             3196793         /m/08yl5d   \n",
       "\n",
       "                                          Movie_name Movie_release_date  \\\n",
       "0                                     Ghosts of Mars         2001-08-24   \n",
       "1  Getting Away with Murder: The JonBenét Ramsey ...         2000-02-16   \n",
       "\n",
       "   Movie_box_office_revenue  Movie_runtime  \\\n",
       "0                14010832.0           98.0   \n",
       "1                       NaN           95.0   \n",
       "\n",
       "                      Movie_languages  \\\n",
       "0  {\"/m/02h40lc\": \"English Language\"}   \n",
       "1  {\"/m/02h40lc\": \"English Language\"}   \n",
       "\n",
       "                             Movie_countries  \\\n",
       "0  {\"/m/09c7w0\": \"United States of America\"}   \n",
       "1  {\"/m/09c7w0\": \"United States of America\"}   \n",
       "\n",
       "                                        Movie_genres  \n",
       "0  {\"/m/01jfsb\": \"Thriller\", \"/m/06n90\": \"Science...  \n",
       "1  {\"/m/02n4kr\": \"Mystery\", \"/m/03bxz7\": \"Biograp...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(movies))\n",
    "movies.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we observe the characters dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450669\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia_movie_ID</th>\n",
       "      <th>Freebase_movie_ID</th>\n",
       "      <th>Movie_release_date</th>\n",
       "      <th>Character_name</th>\n",
       "      <th>Actor_date_of_birth</th>\n",
       "      <th>Actor_gender</th>\n",
       "      <th>Actor_height_meters</th>\n",
       "      <th>Actor_ethnicity_Freebase_ID</th>\n",
       "      <th>Actor_name</th>\n",
       "      <th>Actor_age_at_movie_release</th>\n",
       "      <th>Freebase_character_actor_map_ID</th>\n",
       "      <th>Freebase_character_ID</th>\n",
       "      <th>Freebase_actor_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>975900</td>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Akooshay</td>\n",
       "      <td>1958-08-26</td>\n",
       "      <td>F</td>\n",
       "      <td>1.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wanda De Jesus</td>\n",
       "      <td>42.0</td>\n",
       "      <td>/m/0bgchxw</td>\n",
       "      <td>/m/0bgcj3x</td>\n",
       "      <td>/m/03wcfv7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>975900</td>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Lieutenant Melanie Ballard</td>\n",
       "      <td>1974-08-15</td>\n",
       "      <td>F</td>\n",
       "      <td>1.78</td>\n",
       "      <td>/m/044038p</td>\n",
       "      <td>Natasha Henstridge</td>\n",
       "      <td>27.0</td>\n",
       "      <td>/m/0jys3m</td>\n",
       "      <td>/m/0bgchn4</td>\n",
       "      <td>/m/0346l4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wikipedia_movie_ID Freebase_movie_ID Movie_release_date  \\\n",
       "0              975900         /m/03vyhn         2001-08-24   \n",
       "1              975900         /m/03vyhn         2001-08-24   \n",
       "\n",
       "               Character_name Actor_date_of_birth Actor_gender  \\\n",
       "0                    Akooshay          1958-08-26            F   \n",
       "1  Lieutenant Melanie Ballard          1974-08-15            F   \n",
       "\n",
       "   Actor_height_meters Actor_ethnicity_Freebase_ID          Actor_name  \\\n",
       "0                 1.62                         NaN      Wanda De Jesus   \n",
       "1                 1.78                  /m/044038p  Natasha Henstridge   \n",
       "\n",
       "   Actor_age_at_movie_release Freebase_character_actor_map_ID  \\\n",
       "0                        42.0                      /m/0bgchxw   \n",
       "1                        27.0                       /m/0jys3m   \n",
       "\n",
       "  Freebase_character_ID Freebase_actor_ID  \n",
       "0            /m/0bgcj3x        /m/03wcfv7  \n",
       "1            /m/0bgchn4         /m/0346l4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(characters))\n",
    "characters.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check the summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of plots: 42306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"23890098\\tShlykov, a hard-working taxi driver and Lyosha, a saxophonist, develop a bizarre love-hate relationship, and despite their prejudices, realize they aren't so different after all.\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of plots:', len(summaries))\n",
    "summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem of dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fix typos and absurd dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.loc[movies.Movie_release_date == '1010-12-02','Movie_release_date'] = '2010-12-02'\n",
    "characters.loc[characters.Movie_release_date == '1010-12-02','Movie_release_date'] = '2010-12-02'\n",
    "characters[characters.Actor_date_of_birth == '2050'] = '1971'\n",
    "characters = characters.drop(characters[characters.Actor_date_of_birth < '1500'].index)\n",
    "characters = characters.drop(characters[characters.Actor_date_of_birth > '2030'].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format of movie languages, genres and country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the format of languages, genres, country columns to a simpler format (in terms of utilisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_multiple(chain,deb,step):\n",
    "    '''Split the chain of characters at each \" encountered, and keep only the element in deb +i*step'''\n",
    "    res = chain.split('\"')[deb::step]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.loc[:,'Movie_genres'] = movies.Movie_genres.apply(format_multiple,deb=3,step=4)\n",
    "movies.loc[:,'Movie_countries'] = movies.Movie_countries.apply(format_multiple,deb=3,step=4)\n",
    "movies.loc[:,'Movie_languages'] = movies.Movie_languages.apply(format_multiple,deb=3,step=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13866 movies without Movie_languages (16.96% of the dataset)\n",
      "8154 movies without Movie_countries (9.98% of the dataset)\n",
      "2294 movies without Movie_genres (2.81% of the dataset)\n"
     ]
    }
   ],
   "source": [
    "keys = ['Movie_languages','Movie_countries','Movie_genres']\n",
    "for key in keys:\n",
    "    nb = len(movies[movies[key].apply(len) == 0])\n",
    "    print('{nb} movies without {key} ({percentage:.2f}% of the dataset)'.format(nb=nb,key=key, percentage=nb*100/len(movies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format for dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our study, we only keep the years from the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.Movie_release_date = pd.to_datetime(movies.Movie_release_date,format='%Y-%m-%d').dt.year\n",
    "characters.Movie_release_date = pd.to_datetime(characters.Movie_release_date,format='%Y-%m-%d').dt.year\n",
    "characters.Actor_date_of_birth = pd.to_datetime(characters.Actor_date_of_birth,format='%Y-%m-%d',utc=True,errors='coerce').dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pickle our data in order to reuse directly the cleaned data (and load it faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESTINATION = './Data/'\n",
    "EXT = '.pkl'\n",
    "to_pickle_data = [characters,movies]\n",
    "to_pickle_name = ['characters','movies']\n",
    "for i in range(len(to_pickle_data)):\n",
    "    to_pickle_data[i].to_pickle(DESTINATION+to_pickle_name[i]+EXT)\n",
    "\n",
    "# # To unpickle:\n",
    "# characters = pd.read_pickle(\"./Data/characters.pkl\") \n",
    "# movies = pd.read_pickle(\"./Data/movies.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing the summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lemmatize data (for examples *'is'* becomes *'be'*) to be able to count words better. To do so, we used the `corenlp_plot_summaries` files, and exctracted from it the lemmatized versions of the movies summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to save the data\n",
    "LEMMATIZE_SUMMARIES = False # Takes ~7 mins to run (on i7-10875H CPU)\n",
    "\n",
    "if LEMMATIZE_SUMMARIES:\n",
    "    # Imports\n",
    "    from time import time\n",
    "    import os\n",
    "    import gzip\n",
    "    import re\n",
    "\n",
    "    # Count the number of files in the directory\n",
    "    nb_files = 0\n",
    "    for filename in os.listdir(NLP_FOLDER):\n",
    "        path = os.path.join(NLP_FOLDER, filename)\n",
    "        nb_files += 1\n",
    "    print('Number of summaries:',nb_files)\n",
    "\n",
    "    ext = '.xml.gz' # Extension name\n",
    "    dico_processed_summmaries = {} # Dictionary to store the processed summaries\n",
    "    regex = r'<lemma>.*?</lemma>' # Expression to detect in the corenlp data <lemma>(word)</lemma>\n",
    "\n",
    "    deb = time() # Start timer\n",
    "    count = 0 # Counter\n",
    "\n",
    "    # Iteration over the files\n",
    "    for filename in os.listdir(NLP_FOLDER):\n",
    "        path = os.path.join(NLP_FOLDER, filename) # Path to the file\n",
    "        id_summary = path[len(NLP_FOLDER):-len(ext)] # id of the summary = filename without extension\n",
    "        summary = '' # String to store the summary\n",
    "\n",
    "        if os.path.isfile(path): # Checking if it is a file\n",
    "            with gzip.open(path, 'rb') as f: # Opening the .gz file\n",
    "                for line in f:\n",
    "                    txt = line.decode().strip() # Extract the line as txt\n",
    "                    for elt in re.finditer(regex,txt): # Find all the elements like regex\n",
    "                        summary += re.split('[><]',elt.group(0))[2].lower() + ' ' # Adding only the lemmatized word\n",
    "        \n",
    "        # Set the summary in the dictionary and increment the counter\n",
    "        dico_processed_summmaries[id_summary] = summary\n",
    "        count += 1\n",
    "\n",
    "        # Evolution of the process\n",
    "        if count%1000 == 0:\n",
    "            print('{processed}/{tot} files processed --> {perc:.1f}% ({t:.1f} seconds since deb)'.format(processed=count,tot=nb_files,perc=count/nb_files*100,t=time()-deb))\n",
    "    \n",
    "    # Pickle the file\n",
    "    with open(DATA_FOLDER + 'nlp_summaries.pkl', 'wb') as file:\n",
    "        pickle.dump(dico_processed_summmaries, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to extract the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 236497\n",
      "Summary:\n",
      " king arthur be sit with he knight of the round table , complain about hard time that have befall the kingdom ever since the black knight steal the singing sword . he ask he knight - among they sir osi...\n"
     ]
    }
   ],
   "source": [
    "# Read the pickle file\n",
    "nlp_summaries = pd.read_pickle(DATA_FOLDER+'nlp_summaries.pkl')\n",
    "\n",
    "# Observe the first lemmatized summary\n",
    "for key,value in nlp_summaries.items():\n",
    "    print('Key:',key)\n",
    "    print('Summary:\\n',value[:200]+'...')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating sentences between sexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this part is to separate sentences between sexes to do a sentimental analysis later. To do so, we check if a feminine actor or the *'she'* pronoun is present in a sentence and add them to a new file. We do the same for a male actor and the *'he'* pronoun. Note that for example the sentence *'She hates him'* will become *'she hate he'* once lemmatized, which will be put in the feminine and maculine files\n",
    "\n",
    "This approach is not perfect, since for example in the sentences 'She likes butter. Indeed, the actress loves food.', only the first one will be added. It is not perfect, but the best solution we could think of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the characters\n",
    "characters_per_film = characters.copy()\n",
    "# Put the column in their correct type and lower chars\n",
    "characters_per_film['Wikipedia_movie_ID'] = characters_per_film['Wikipedia_movie_ID'].astype(int)\n",
    "characters_per_film['Character_name'] = characters_per_film['Character_name'].astype(str).apply(lambda x: x.lower())\n",
    "# Sort the dataframe by movie ID\n",
    "characters_per_film = characters_per_film.sort_values(by=['Wikipedia_movie_ID'])\n",
    "# Drio rows where the character name or the gender is empty\n",
    "characters_per_film = characters_per_film.dropna(subset=['Character_name', 'Actor_gender'])\n",
    "# Group the dataframe by movie ID\n",
    "characters_per_film = characters_per_film.groupby('Wikipedia_movie_ID')[['Wikipedia_movie_ID', 'Character_name', 'Actor_gender']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>plot_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12205</th>\n",
       "      <td>330</td>\n",
       "      <td>in order to prepare the role of a important ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23703</th>\n",
       "      <td>3217</td>\n",
       "      <td>after be pull through a time portal , ash will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36696</th>\n",
       "      <td>3333</td>\n",
       "      <td>the film follow two juxtapose family : the nor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40296</th>\n",
       "      <td>3746</td>\n",
       "      <td>-lcb- -lcb- hatnote -rcb- -rcb- in los angeles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25811</th>\n",
       "      <td>3837</td>\n",
       "      <td>in the american old west of 1874 , constructio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                    plot_lemmatized\n",
       "12205   330  in order to prepare the role of a important ol...\n",
       "23703  3217  after be pull through a time portal , ash will...\n",
       "36696  3333  the film follow two juxtapose family : the nor...\n",
       "40296  3746  -lcb- -lcb- hatnote -rcb- -rcb- in los angeles...\n",
       "25811  3837  in the american old west of 1874 , constructio..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataframe from lemmatized summaries\n",
    "df = pd.DataFrame(list(nlp_summaries.items()), columns = ['id','plot_lemmatized'])\n",
    "# Put column in their correct type\n",
    "df['id'] = df['id'].astype(int)\n",
    "# Sort the dataframe by movie ID\n",
    "df = df.sort_values(by=['id'])\n",
    "# Show the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 files processed\n",
      "2000 files processed\n",
      "3000 files processed\n",
      "4000 files processed\n",
      "5000 files processed\n",
      "6000 files processed\n",
      "7000 files processed\n",
      "8000 files processed\n",
      "9000 files processed\n",
      "10000 files processed\n",
      "11000 files processed\n",
      "12000 files processed\n",
      "13000 files processed\n",
      "14000 files processed\n",
      "15000 files processed\n",
      "16000 files processed\n",
      "17000 files processed\n",
      "18000 files processed\n",
      "19000 files processed\n",
      "20000 files processed\n",
      "21000 files processed\n",
      "22000 files processed\n",
      "23000 files processed\n",
      "24000 files processed\n",
      "25000 files processed\n",
      "26000 files processed\n",
      "27000 files processed\n",
      "28000 files processed\n",
      "29000 files processed\n",
      "30000 files processed\n",
      "31000 files processed\n",
      "32000 files processed\n",
      "33000 files processed\n",
      "34000 files processed\n",
      "35000 files processed\n",
      "36000 files processed\n",
      "37000 files processed\n",
      "38000 files processed\n",
      "39000 files processed\n",
      "40000 files processed\n",
      "41000 files processed\n",
      "42000 files processed\n",
      "43000 files processed\n",
      "44000 files processed\n",
      "45000 files processed\n",
      "46000 files processed\n",
      "47000 files processed\n",
      "48000 files processed\n",
      "49000 files processed\n",
      "50000 files processed\n",
      "51000 files processed\n",
      "52000 files processed\n",
      "53000 files processed\n",
      "54000 files processed\n",
      "55000 files processed\n",
      "56000 files processed\n",
      "57000 files processed\n",
      "58000 files processed\n",
      "59000 files processed\n",
      "60000 files processed\n",
      "61000 files processed\n",
      "62000 files processed\n",
      "63000 files processed\n"
     ]
    }
   ],
   "source": [
    "# Set to True to save the data\n",
    "SEPARATE_SENTENCES = False # Takes ~20 mins to run (on i7-10875H CPU)\n",
    "\n",
    "if SEPARATE_SENTENCES:\n",
    "    # Imports\n",
    "    count = 0\n",
    "    dico_male = {}\n",
    "    dico_female = {}\n",
    "\n",
    "    # Loop on subgroups\n",
    "    for _, group in characters_per_film:\n",
    "        # Get the movie id\n",
    "        movie_id = group['Wikipedia_movie_ID'].iloc[0]\n",
    "        female_sentences = []\n",
    "        male_sentences = []\n",
    "\n",
    "        # Check if wikipedia movie id is in the nlp summaries\n",
    "        if movie_id in df['id'].values:\n",
    "            index = df[df['id'] == movie_id].index[0] # Take the correct index\n",
    "            plot = df['plot_lemmatized'][index] # Take the correct plot\n",
    "            sentences = plot.split('.') # Split into sentences\n",
    "            # Loop on sentences\n",
    "            for sentence in sentences:\n",
    "                # Loop on characters\n",
    "                for character in group['Character_name']:\n",
    "                    # Find the sex of the character\n",
    "                    gender = group[group['Character_name'] == character].Actor_gender.values[0]\n",
    "                    # Find potential pronouns discriminative on gender\n",
    "                    he_index = sentence.find('he')\n",
    "                    she_index = sentence.find('she')\n",
    "                    # Check if the pronoun or actor name is in the sentence\n",
    "                    if ((character in sentence) or (she_index >= 0) or (he_index >= 0)):\n",
    "                        # Store in dictionary depending on gender of sentence (can also be in both)\n",
    "                        if ((gender== '1') or (she_index >= 0)):\n",
    "                            female_sentences.append(sentence)\n",
    "                        if ((gender == '0') or (he_index >= 0)):\n",
    "                            male_sentences.append(sentence)\n",
    "\n",
    "        # Store in dictionary and increment counter\n",
    "        dico_male[movie_id] = male_sentences\n",
    "        dico_female[movie_id] = female_sentences\n",
    "        count += 1\n",
    "\n",
    "        # Evolution of the process\n",
    "        if count%1000 == 0:\n",
    "            print('{processed} files processed'.format(processed=count))\n",
    "\n",
    "    # Pickle the file\n",
    "    with open(DATA_FOLDER + 'male_sentences.pkl', 'wb') as file:\n",
    "        pickle.dump(dico_male, file, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "    with open(DATA_FOLDER + 'female_sentences.pkl', 'wb') as file:\n",
    "        pickle.dump(dico_female, file, protocol=pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriching the CMU dataset with IMDb dataset\n",
    "\n",
    "## Loading the data and first glimpse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the most useful datasets for the moment\n",
    "TITLE_BASICS_DATASET = DATA_FOLDER + 'title.basics.tsv.gz'\n",
    "TITLE_RATINGS_DATASET = DATA_FOLDER + 'title.ratings.tsv.gz'\n",
    "\n",
    "columns_title_basics = ['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult', 'startYear', 'endYear', 'runtimeMinutes', 'genres']\n",
    "columns_ratings = ['tconstIdentifier', 'averageRating', 'numVotes']\n",
    "\n",
    "CLEAN_DATA = False # True to clean again the data, False to use the already pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alice\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3418: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of title_basics:  9363391\n"
     ]
    }
   ],
   "source": [
    "if CLEAN_DATA:\n",
    "    #Load title_basics\n",
    "    title_basics = load_metadata(TITLE_BASICS_DATASET, column_names=columns_title_basics)\n",
    "    print(\"length of title_basics: \", len(title_basics))\n",
    "    title_basics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of ratings:  1246149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alice\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3418: DtypeWarning: Columns (1,2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "if CLEAN_DATA:\n",
    "    #Load title_ratings\n",
    "    ratings = load_metadata(TITLE_RATINGS_DATASET, column_names=columns_ratings)\n",
    "    print(\"length of ratings: \", len(ratings))\n",
    "    ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_DATA:\n",
    "    #Create a new table with only titleType=movies (get rid of videos, tvshows, tvepisodes and short)\n",
    "    title_basics_movies = title_basics[title_basics[\"titleType\"] == \"movie\"]\n",
    "    #Remove the endYear column since movies are not concerned by thats\n",
    "    title_basics_movies_cleaned = title_basics_movies.drop(columns='endYear')\n",
    "    title_basics_movies_cleaned.replace('\\\\N',np.NaN,inplace=True) # replace \\N by NaN\n",
    "    # datetime format for dates\n",
    "    title_basics_movies_cleaned.startYear = pd.to_datetime(title_basics_movies_cleaned.startYear,format='%Y').dt.year \n",
    "    title_basics_movies_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_DATA:\n",
    "    #Pickle the data\n",
    "    to_pickle_data = title_basics_movies_cleaned\n",
    "    to_pickle_name = 'IMDb_title_movies'\n",
    "    to_pickle_data.to_pickle(DESTINATION+to_pickle_name+EXT)\n",
    "\n",
    "if not CLEAN_DATA: # for testing part\n",
    "    # load already pickled data\n",
    "    title_basics_movies_cleaned = pd.read_pickle(\"./Data/IMDb_title_movies.pkl\")\n",
    "    title_basics_movies_cleaned.startYear = pd.to_datetime(title_basics_movies_cleaned.startYear,format='%Y').dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging IMDb and CMU datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We match the movies from one dataset to the films on the other dataset on the movie name, as the ids are different.\n",
    "\n",
    "In order to avoid mismatched pairs due to a little variation in the titles, we matched films of the same year, with almost identical titles. We create a dictionnary that matches the index of matched films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_IMDb = title_basics_movies_cleaned.copy()\n",
    "copy_IMDb = copy_IMDb[copy_IMDb.startYear >= 1910]\n",
    "copy_CMU = movies.copy()\n",
    "copy_CMU.dropna(subset=['Movie_box_office_revenue', 'Movie_release_date'], inplace=True)\n",
    "copy_IMDb.dropna(subset= ['startYear'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8328\n",
      "541993\n"
     ]
    }
   ],
   "source": [
    "print(len(copy_CMU))\n",
    "print(len(copy_IMDb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "common_words = {'a','an','and','the','of','at','in'}\n",
    "punctuation = {'.',',','!',';','?',''}\n",
    "def compare(df1,df2,col1_title,col2_title,col1_year,col2_year,threshold = 0.8, delta_year=1):\n",
    "    matched = {}\n",
    "    count = 0\n",
    "    for idx1,row1 in df1.iterrows():\n",
    "        title1 = set(re.split('[ :,]',row1[col1_title].lower()))\n",
    "        title1 = title1.difference(punctuation)\n",
    "        y1 = row1[col1_year]\n",
    "        #for idx2,row2 in df2[df2[col2_year].isin([y1-delta_year+i for i in range(delta_year*2)])].iterrows():\n",
    "        for idx2,row2 in df2[df2[col2_year]==y1].iterrows():\n",
    "            title2 = set(re.split('[ :,]',row2[col2_title].lower()))\n",
    "            title2 = title2.difference(punctuation)\n",
    "            if len(title1 & title2)/(len(title1 | title2)) > threshold:\n",
    "                try:\n",
    "                    matched[idx1].append(idx2)\n",
    "                except KeyError:\n",
    "                    matched[idx1] = [idx2]\n",
    "        count += 1\n",
    "        if count == 100: # remove for the whole computation\n",
    "            break\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of execution: 71.45266842842102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [218707],\n",
       " 7: [29325],\n",
       " 13: [95308],\n",
       " 17: [57208],\n",
       " 29: [244954],\n",
       " 36: [32552],\n",
       " 49: [79055],\n",
       " 53: [388561],\n",
       " 54: [95478],\n",
       " 61: [95595],\n",
       " 85: [13738],\n",
       " 88: [49725],\n",
       " 99: [61508],\n",
       " 119: [460855],\n",
       " 124: [3795944],\n",
       " 126: [62367],\n",
       " 131: [77543],\n",
       " 132: [45916],\n",
       " 140: [112030],\n",
       " 142: [75718],\n",
       " 151: [85085],\n",
       " 164: [116042],\n",
       " 174: [450732],\n",
       " 175: [66426],\n",
       " 197: [358897],\n",
       " 201: [29395],\n",
       " 202: [75978],\n",
       " 222: [399377],\n",
       " 233: [76270],\n",
       " 238: [105516],\n",
       " 251: [838312],\n",
       " 261: [76672],\n",
       " 281: [117496],\n",
       " 289: [100197],\n",
       " 298: [391332],\n",
       " 305: [101602],\n",
       " 306: [116950],\n",
       " 310: [85360],\n",
       " 318: [138155],\n",
       " 321: [99115],\n",
       " 326: [299976],\n",
       " 346: [117868],\n",
       " 357: [108120],\n",
       " 362: [44117],\n",
       " 367: [91899],\n",
       " 370: [448374],\n",
       " 391: [4720360],\n",
       " 431: [113744],\n",
       " 464: [3200213],\n",
       " 473: [158619],\n",
       " 487: [105431],\n",
       " 498: [27711],\n",
       " 523: [765915],\n",
       " 536: [353187],\n",
       " 546: [353226],\n",
       " 549: [110870],\n",
       " 551: [3391684, 4535866],\n",
       " 555: [1006903],\n",
       " 562: [418100],\n",
       " 575: [107409],\n",
       " 576: [428090],\n",
       " 595: [775146],\n",
       " 599: [116360],\n",
       " 605: [42029],\n",
       " 607: [117887],\n",
       " 612: [61556],\n",
       " 621: [44275],\n",
       " 623: [58112, 58113],\n",
       " 625: [1396754],\n",
       " 628: [320951],\n",
       " 633: [94138],\n",
       " 638: [4440389],\n",
       " 646: [48263],\n",
       " 650: [90495],\n",
       " 657: [4187666],\n",
       " 673: [21883],\n",
       " 677: [2704387],\n",
       " 739: [788500],\n",
       " 771: [41587],\n",
       " 776: [2585015],\n",
       " 794: [70490],\n",
       " 796: [57259],\n",
       " 797: [116502],\n",
       " 822: [88097],\n",
       " 824: [384980],\n",
       " 839: [69732],\n",
       " 846: [4865099],\n",
       " 848: [2850301],\n",
       " 864: [104717],\n",
       " 890: [2488538],\n",
       " 920: [84469],\n",
       " 927: [454407]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "deb = time()\n",
    "matched = compare(copy_CMU,copy_IMDb, 'Movie_name', 'primaryTitle', 'Movie_release_date', 'startYear')\n",
    "end = time()\n",
    "print('Time of execution:', end-deb)\n",
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the matching table\n",
    "with open(DATA_FOLDER + 'matching_table.pkl', 'wb') as file:\n",
    "        pickle.dump(matched, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
